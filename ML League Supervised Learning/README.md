# ML League Supervised Learning Competition

Welcome to the **ML League** competition hosted by the **Web Enthusiasts Club**!


## üîó Competition Link

You can participate in the competition and submit your predictions here:
[https://www.kaggle.com/competitions/ml-league-supervised-learning-competition](https://www.kaggle.com/competitions/ml-league-supervised-learning-competition)


## üèÜ Competition Overview

* Train your machine learning models using the provided dataset on Kaggle.
* Submit your predictions on the test dataset to Kaggle.
* The leaderboard updates dynamically based on your submission accuracy.
* The final rankings will be declared after the competition ends.
* Top performers will be rewarded based on both Kaggle leaderboard ranking and code quality.


## üìã Submission Instructions

1. **Clone the base repository**
   First, clone the official ML League repository:

   ```bash
   git clone https://github.com/WebClub-NITK/ML-League-2025.git
   cd ML-League-2025/ML League Supervised Learning
   ```

2. **Create your submission folder**
   Inside the `ML League Supervised Learning` directory, create a **new folder named after your roll number**.
   For example:
   ```bash
   mkdir 23CV126 // Your Roll No.
   ```

   All your Jupyter notebooks and code files **must be placed inside this folder**. This is **mandatory** for final submission.

3. **Train and optimize your model**
   Use the provided Kaggle dataset to train and fine-tune your model for the best possible performance.

4. **Submit predictions on Kaggle**
   Submit your predictions on Kaggle and note your position on the leaderboard.

5. **Push your complete code to GitHub**
   Push your final project to your GitHub repository. Ensure:

   * Your code is clean and well-documented
   * Everything is inside the `ML League Supervised Learning/YOUR_ROLL_NO` folder

6. **Add collaborator**
   Add me (**GitHub username:** `saketjha34`) as a **collaborator** on your GitHub repository.

7. **Send final email**
   Once you're done, **email me** at:
   `jhasaketsunil.231cv126@nitk.edu.in`
   Make sure to include your **GitHub repository link** in the email.


## üìë Code Requirements

Your submission must include:

* **Well-defined functions** and clear, readable comments explaining each part of the code.
* Titles and subtitles (e.g., markdown cells in notebooks) describing your workflow step-by-step.
* Usage of the techniques and concepts covered in the **Linear-Regression.ipynb** and **Advanced-ML-Techniques.ipynb** files.
* Creative use of other advanced techniques such as:

  * Hyperparameter tuning (e.g., with Optuna)
  * Feature engineering
  * Model ensembling
  * Cross-validation strategies
  * Any other relevant ML methods to improve performance.


## üßÆ Evaluation Criteria

| Aspect                  | Weightage                  |
| ----------------------- | -------------------------- |
| Kaggle leaderboard rank | Primary factor             |
| Code quality            | Clear structure & comments |
| Innovation              | Use of advanced techniques |
| Documentation           | Explanations & readability |

Final rankings will consider both **Kaggle leaderboard position** and **submitted code quality**.


## üîß Resources

* **Starter notebooks**:

  * `linear-regression.ipynb` *(for basic workflow understanding)*
  * `Advanced-ML-Techniques.ipynb` *(key concepts and techniques)*
  * `Clustering-KMeans.ipynb` *Starter Notebook for Unsupervised Learning*

* **Official documentation and guides**:

  * [scikit-learn (sklearn) documentation](https://scikit-learn.org/stable/documentation.html) ‚Äî essential for machine learning models and utilities
  * [Optuna documentation](https://optuna.org/) ‚Äî powerful hyperparameter tuning framework
  * [pandas documentation](https://pandas.pydata.org/docs/) ‚Äî for data manipulation and analysis
  * [Feature engineering resources](https://www.kaggle.com/learn/feature-engineering) ‚Äî practical tips and techniques to create better features


## üí° Tips for Success

* Start with exploratory data analysis to understand your data.
* Try baseline models and gradually improve.
* Experiment with feature engineering and hyperparameter tuning.
* Keep your code modular and well-commented.
* Document every step with titles and explanations.
* Be creative and think beyond the basics!


Looking forward to your innovative solutions and exciting participation!

Good luck,
**Web Enthusiasts Club**
Contact: [jhasaketsunil.231cv126@nitk.edu.in](mailto:jhasaketsunil.231cv126@nitk.edu.in)