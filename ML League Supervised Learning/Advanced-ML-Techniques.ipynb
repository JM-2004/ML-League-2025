{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfbd3d0a",
   "metadata": {},
   "source": [
    "# 📚 Homework & Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f67900",
   "metadata": {},
   "source": [
    "## 1. 📊 Feature Selection Using Covariance\n",
    "\n",
    "- **Covariance Matrix** helps us understand how variables vary together.\n",
    "- **High positive/negative covariance** means features may be linearly related.\n",
    "- Use `df.corr()` or `sns.heatmap(df.corr(), annot=True)` for plotting.\n",
    "- Helps in identifying **multicollinearity** which can harm model performance.\n",
    "- Remove or combine highly correlated features.\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "corr_matrix = df.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.show()\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ddbd3a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c328c081",
   "metadata": {},
   "source": [
    "# 2. 🧼 Handling NaNs & Imputation Strategies\n",
    "\n",
    "### Dealing with Missing Values:\n",
    "\n",
    "* **Drop**: if rows or columns have too many NaNs.\n",
    "* **Impute**:\n",
    "\n",
    "  * Mean/Median: for numerical columns\n",
    "  * Mode: for categorical columns\n",
    "  * KNN or Model-based: for advanced imputation\n",
    "\n",
    "```python\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')  # or 'median', 'most_frequent'\n",
    "df[numeric_cols] = imputer.fit_transform(df[numeric_cols])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3754a120",
   "metadata": {},
   "source": [
    "### Scaling:\n",
    "\n",
    "* Normalize/scale numeric values for models that are sensitive to scale (like linear regression).\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353e184c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79d9b15f",
   "metadata": {},
   "source": [
    "## 3. 🔖 Label Encoding\n",
    "\n",
    "* Converts **categorical** data into numerical form.\n",
    "* Use **LabelEncoder** for ordinal values (like low, medium, high).\n",
    "* For nominal data, use **OneHotEncoding**.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['category_encoded'] = le.fit_transform(df['category'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ff197d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d93a7b6",
   "metadata": {},
   "source": [
    "## 4. 🌲 ML Models: RandomForest, GradientBoosting, XGBoost, CatBoost\n",
    "\n",
    "### Overview:\n",
    "\n",
    "| Model            | Strengths                                           |\n",
    "| ---------------- | --------------------------------------------------- |\n",
    "| Random Forest    | Robust, fast, handles non-linear data               |\n",
    "| GradientBoosting | Accurate, slower, prone to overfitting              |\n",
    "| XGBoost          | Optimized GBM with regularization, very fast        |\n",
    "| CatBoost         | Great with categorical features, no encoding needed |\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100)\n",
    "xgb = XGBRegressor(learning_rate=0.1, n_estimators=100)\n",
    "cat = CatBoostRegressor(verbose=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e69c947",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47403957",
   "metadata": {},
   "source": [
    "## 5. 📏 Evaluation Metrics for Regression\n",
    "\n",
    "| Metric                    | Use When You Want To…                             |\n",
    "| ------------------------- | ------------------------------------------------- |\n",
    "| MAE (Mean Absolute Error) | Measure average error, less sensitive to outliers |\n",
    "| MSE (Mean Squared Error)  | Penalize large errors more heavily                |\n",
    "| RMSE                      | Similar to MSE, interpretable in original units   |\n",
    "| R² Score                  | See how much variance is explained by model       |\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9d06ae",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f5f7090",
   "metadata": {},
   "source": [
    "## 6. 🔧 Hyperparameter Tuning for Boosting Models\n",
    "\n",
    "* Use **GridSearchCV** or **RandomizedSearchCV** for tuning.\n",
    "* Common parameters:\n",
    "\n",
    "  * `learning_rate`\n",
    "  * `n_estimators`\n",
    "  * `max_depth`\n",
    "  * `subsample`, `colsample_bytree` (for XGBoost)\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'max_depth': [3, 5, 7], 'learning_rate': [0.01, 0.1]}\n",
    "grid = GridSearchCV(XGBRegressor(), param_grid, cv=3)\n",
    "grid.fit(X_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a81be37",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5db48d4",
   "metadata": {},
   "source": [
    "\n",
    "## 7. 🧠 Advanced Feature Engineering\n",
    "\n",
    "### Custom Capacity Extraction Function\n",
    "\n",
    "```python\n",
    "def get_capacity(x: pandas.Series, df: pandas.DataFrame) -> pandas.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts 'X.XL' capacity (e.g., '2.0L') from a Series of text and adds it as a float column.\n",
    "    \"\"\"\n",
    "    capacity = []\n",
    "    for text in x:\n",
    "        tokens = text.split()\n",
    "        if len(tokens) >= 2 and tokens[1].endswith('L'):\n",
    "            try:\n",
    "                cap_val = float(tokens[1].strip('L'))\n",
    "                capacity.append(cap_val)\n",
    "            except:\n",
    "                capacity.append(np.nan)\n",
    "        else:\n",
    "            capacity.append(np.nan)\n",
    "    df['capacity'] = capacity\n",
    "    return df\n",
    "```\n",
    "\n",
    "* This function is helpful when features are **embedded in text**.\n",
    "* Always clean and extract key features before modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93628a69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
